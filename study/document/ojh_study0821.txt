gradient exploding / gradient vanishing 정리

- gradient vanishing
깊은 인공 신경망을 학습하다보면 역전파 과정에서 입력층으로 갈수록 기울기(gradient)가 점차적으로 작아지는 현상이 발생할 수 있다. 입력층에 가까운 층들에서 가중치들이 업데이트가 제대로 되지 않으면 결국 최적의 모델을 찾을 수 없게 되는 현상이다.
gradient vanishing으로 인해 학습이 잘 안되는 것을 Underfitting(과소적합)이라고 한다.

- gradient exploding
반대로 학습과정(역전파과정)에서 기울기(gradient)가 점차 커지더니 가중치들이 기하급수적으로 큰 값이 되면서 결국 발산되어 학습이 제대로 이루어지지 않는 현상이다.