feature importance

-> 예측하는 값에 대해 중요한 변수는 무엇이고, 어떤 영향을 끼치고 있는지 파악하는 것
-> 선형회귀의 경우 중요한 변수는 Coefficient와 p-value를 통해 파악할 수 있다.
-> Tree 기반의 모델을 이용하여 중요 feature를 추출할 수 있다.

-> feature importance 측정 기준
1) weight : 변수 별로 데이터를 분리하는 데 쓰인 횟수를 측정 기준으로 둔다.
2) cover : 변수가 쓰여서 데이터가 분리되었을 때의 데이터 수로 가중치를 준 값을 측정 기준으로 둔다.
3) gain : 변수를 사용했을 때, 줄어드는 평균적인 training loss값을 측정 기준으로 둔다.

--------------------------------------------------------------------------

feature engineering

-> feature importance를 기반으로 feature selection을 하는 것, 즉 예측하는 값에 중요한 변수를 선정하고, 중요하지 않은 변수를 버리는 작업이다.

-> 중요한 변수가 없다면 만들어서 넣어주는 작업도 포함한다.

-> 모델 성능에 미치는 영향이 크기 때문에 머신러닝 응용에 있어서 굉장히 중요한 단계이며, 전문성과 시간과 비용이 많이 드는 작업이다.

-> 특징 선택 : 트리 기반 모델로 feature importance를 측정
-> 차원 감소, 특징 추출 : 대표적으로 PCA를 사용
-> 특징 생성 또는 구축 : 도메인(분야) 전문성을 이용한다.